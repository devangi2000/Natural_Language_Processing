# Natural_Language_Processing

This is an implementation of LSTM (Long-short term memory) network, which is a special kind of RNN, capable of learning long-term dependencies. This model generates words like Shakespeare, after being trained on a sample. TensorFlow has been used with Keras as the backend.

During the project implementation, I learnt about LSTMs and their applications. They can not only process single data points (such as images), but also entire data sequences (like speech or video). For instance, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic. An LSTM is a particular type of RNN with a mechanism to avoid the vanishing gradient problem and learn long term dependencies. It is still capable of learning short term dependencies, hence the name "Long-Short Term Memory".

LSTMs are an unsupervised learning method, although technically, they are trained using supervised learning methods
